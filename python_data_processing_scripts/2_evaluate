import csv
import pandas as pd
from sklearn.metrics import f1_score, precision_score, recall_score, classification_report
from groq import Groq
from tqdm import tqdm

# === CONFIG ===
csv_path = r"C:\Users\domin\OneDrive\Documents\Elderly_Hackathon\sampled_messages.csv"
output_predictions = "gemma2_9b_it_predictions_results.csv"
output_metrics = "gemma2_9b_it_evaluation_metrics.csv"
MODEL = "gemma2-9b-it"

# === Groq Client Init ===
client = Groq(api_key="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")  # Replace with your API key

def groq_predict_label(text):
    """Use Groq LLM to predict label from text."""
    prompt = f"""
    Given the following message, classify it as one of: spam, ham, or Smishing.
    Return only the label, nothing else.

    Message: {text}
    """
    response = client.chat.completions.create(
        messages=[{"role": "user", "content": prompt}],
        model=MODEL
    )
    label = response.choices[0].message.content.strip().lower()
    if "spam" in label:
        return "spam"
    elif "ham" in label:
        return "ham"
    elif "smishing" in label:
        return "Smishing"
    else:
        return "spam"  # Default fallback

# === Load CSV ===
print("üìÇ Loading dataset...")
data = pd.read_csv(csv_path)
print(f"‚úÖ {len(data)} messages loaded.")

# === Predict Labels ===
tqdm.pandas(desc="üîç Predicting")
print("ü§ñ Predicting message labels using Groq...")
data['PREDICTED'] = data['TEXT'].progress_apply(groq_predict_label)

# === Evaluate Performance ===
print("üìä Evaluating performance...")
y_true = data['LABEL'].str.lower()
y_pred = data['PREDICTED'].str.lower()

# Overall (macro avg)
overall_f1 = f1_score(y_true, y_pred, average='macro')
overall_precision = precision_score(y_true, y_pred, average='macro')
overall_recall = recall_score(y_true, y_pred, average='macro')

# Per-class scores
report = classification_report(
    y_true,
    y_pred,
    output_dict=True,
    zero_division=0  # Avoid division errors if class missing
)

# Format results for CSV
metrics_rows = [
    {"Label": "OVERALL", "Precision": overall_precision, "Recall": overall_recall, "F1-Score": overall_f1}
]
for label, scores in report.items():
    if label in ["accuracy", "macro avg", "weighted avg"]:
        continue
    metrics_rows.append({
        "Label": label,
        "Precision": scores["precision"],
        "Recall": scores["recall"],
        "F1-Score": scores["f1-score"]
    })

# === Save Predictions ===
data.to_csv(output_predictions, index=False)
print(f"üíæ Predictions saved to {output_predictions}")

# === Save Evaluation Metrics ===
metrics_df = pd.DataFrame(metrics_rows)
metrics_df.to_csv(output_metrics, index=False)
print(f"üìä Evaluation metrics saved to {output_metrics}")
